{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document indexing with LSH Forest\n",
    "\n",
    "Practical course material for the ASDM Class 09 (Text Mining) by Florian Leitner.\n",
    "\n",
    "© 2016 Florian Leitner. All rights reserved.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Today's lab will cover the two most fundamental techniques of Text Mining: Document classification and indexing; This notebook covers the latter (\"building the world's fastest search engine\").\n",
    "\n",
    "For background, please check the \"Pitfalls for budding data scientists\" and the \"Shingling and Jaccard's similarity\" notebooks.\n",
    "\n",
    "An excellent resource for basic LSH is the free book (PDF) by Rajaraman, [\"Mining of Massive Datasets\"](http://infolab.stanford.edu/~ullman/mmds/book.pdf). We will specifically be looking at a \"self-tuning\" LSH design, the [LSH Forest by Bawa et al.](http://infolab.stanford.edu/~bawa/Pub/similarity.pdf) Another design variant of this hashing technique is [LSH Ensemble by Zhu et al.](http://www.vldb.org/pvldb/vol9/p1185-zhu.pdf), which is designed to use LSH on a short (query) sequence lookup against long (document) sequences by modifying similarity to only use the length of the query in the denominator. LSH Ensemble is therefore is a nice simple approach to an ranked IR system \"on the cheap\" and without having to fall back on a pairwise ranking strategy to come up with the most similar matches to the query first. But as we will be using *documents* as our queries, though, we will focus on LSH Forest here.\n",
    "\n",
    "Ensure you have **NLTK** installed or install it with:\n",
    "\n",
    "- Anaconda Python: `conda install nltk`\n",
    "- Stock Python: `pip3 install nltk`\n",
    "\n",
    "Also install the `segtok` tokenization and segmentation library created by yours truly (the instructor, me):\n",
    "\n",
    "- Anaconda Python: `conda install segtok`\n",
    "- Stock Python: `pip3 install segtok`\n",
    "\n",
    "Once done, the following imports should work; If not, you can always skip the practical document indexing examples below.\n",
    "\n",
    "Once done, the following imports should work; If not, you can always skip the practical example at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import segtok\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have at least NLTK book (and/or popular) downloaded and installed from the dialog window that shows up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline --no-import-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard's set similarity\n",
    "\n",
    "(see the relevant Notebook for a detailed discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard(seq1, seq2):\n",
    "    \"\"\"\n",
    "    The Jaccard similarity between two sequences:\n",
    "    |∩(X,Y)| / |∪(X,Y)|\n",
    "    \"\"\"\n",
    "    \n",
    "    x = set(seq1)\n",
    "    y = set(seq2)\n",
    "    return len(x & y) / len(x | y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-hash signatures\n",
    "\n",
    "(see the \"Pitfalls...\" Notebook for a discussion of `hash` vs. `id`)\n",
    "\n",
    "We will look into an implementation of LSH by [Christian Jauvin](http://cjauvin.github.io/); see [github.com/go2starr/lshhdc](https://github.com/go2starr/lshhdc) for educational puposes (but note that SciKit-Learn will provide us with the implementation we will be using)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MinHashSignature:\n",
    "    \"\"\"Hash signatures for sets/tuples using minhash.\"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Define the size of the hash pool\n",
    "        (number of hash functions).\n",
    "        \"\"\"\n",
    "        \n",
    "        self.size = size\n",
    "        self.hashes = self.__gen_hash_functions()\n",
    "\n",
    "    def __gen_hash_functions(self):\n",
    "        \"\"\"\n",
    "        Return a list of `size` different hash functions.\n",
    "        \"\"\"\n",
    "        \n",
    "        def hash_factory(mutator):\n",
    "            # note that each element in \n",
    "            # an item from a seq being min-hashed\n",
    "            # will be pushed in here\n",
    "            # I.e., item typically is a token,\n",
    "            # character, n-gram, etc.\n",
    "            return lambda item: hash(\n",
    "                \"salt\" + str(mutator) + str(item) + \"salt\"\n",
    "            )\n",
    "        \n",
    "        return [ hash_factory(m) for m in range(self.size) ]\n",
    "\n",
    "    def sign(self, seq):\n",
    "        \"\"\"\n",
    "        Return the min-hash signatures\n",
    "        for any iterable input `seq`.\n",
    "        \"\"\"\n",
    "        \n",
    "        signature = [ float(\"inf\") ] * self.size\n",
    "        \n",
    "        for idx, hash_function in enumerate(self.hashes):\n",
    "            # min-hashing the input seq:\n",
    "            # for each hash_function generated,\n",
    "            # choose the hash value (ID)\n",
    "            # (from hashing all the input sequence\n",
    "            # with that hash function)\n",
    "            # that has the smallest (i.e., min) value\n",
    "            # as the value for the final signature\n",
    "            signature[idx] = min(hash_function(item) for item in seq)\n",
    "        \n",
    "        return signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a min-hash signature instance that generates four different hash values (signature size). This signature can be understood as generating four different, random orderings of the (whole) \"possible\" input vocabulary (if you like, \"hash-permuted row IDs\" of your complete vocabulary). The signature therefore reports the first matching token in the input sequence for each of the size different orderings over the whole vocabulary (not just the words in that document!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-7706907317463682804,\n",
       " -5605639870768415415,\n",
       " -6223539463645503589,\n",
       " -6589356995171225446]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig4 = MinHashSignature(4)\n",
    "sig4.sign([\"this\", \"is\", \"an\", \"example\", \"document\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banded Locality Sensitive Hashing\n",
    "\n",
    "(see the \"Pitfalls...\" Notebook for a discussion of floating point issues with probabity values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BandedLSH:\n",
    "    # n. bands (n. hash tables):\n",
    "    # b = dictionary.hasher.size /\n",
    "    #     dictionary.hasher.bandwidth\n",
    "    #    \n",
    "    # n. hashes per band:\n",
    "    # r = dictionary.hasher.bandwidth \n",
    "    \n",
    "    def __init__(self, size, threshold):\n",
    "        \"\"\"\n",
    "        Set up targeting a Jaccard similarity `threshold`\n",
    "        using the given min-hash signature `size`.\n",
    "        \"\"\"\n",
    "        \n",
    "        # total size (n. rows) of the matrix\n",
    "        # (slide \"Banded locality sensitve hasing\")\n",
    "        self.size = size\n",
    "        # *desired* Jaccard similarity threshold\n",
    "        # of the items belonging together\n",
    "        self.threshold = threshold\n",
    "        # number hashes to use per band\n",
    "        # to get as close to that threshold as possible\n",
    "        # bandwidth is the \"rows per band\"\n",
    "        self.bandwidth = self.get_bandwidth(\n",
    "            size,\n",
    "            threshold)\n",
    "        self.min_hasher = MinHashSignature(size)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bandwidth(n, t):\n",
    "        \"\"\"\n",
    "        Approximate the bandwidth\n",
    "        (number of rows in each band)\n",
    "        for a fixed total number of\n",
    "        rows `n` (bandwith * n. bands)\n",
    "        needed to achieve the desired\n",
    "        similarity threshold `t`.\n",
    "\n",
    "        Similarity threshold t = (1. / b) ** (1. / r)\n",
    "        with size (total rows in LSH) n = b * r\n",
    "        where:\n",
    "        b = number of bands\n",
    "        r = number of rows per band\n",
    "        therefore:\n",
    "        b = 1 / (t ** r)\n",
    "        and:\n",
    "        r = n /b\n",
    "        \"\"\"\n",
    "        \n",
    "        best = n # default: 1 band with `n` rows\n",
    "        minerr = float(\"inf\") # default: \"infinite\" error\n",
    "        \n",
    "        # Try finding the smallest possible bandwith r\n",
    "        # by starting with one row per band, etc:\n",
    "        for r in range(1, n + 1):\n",
    "            try:\n",
    "                # calculate the number of bands b needed to\n",
    "                # achieve the desired threshold t with the\n",
    "                # current number of rows r\n",
    "                b = 1. / (t ** r)\n",
    "            except: # Divide by zero -> `size` is too big\n",
    "                return best\n",
    "            \n",
    "            # how close would the calculated number bands b\n",
    "            # with the current bandwith r, \n",
    "            # required to achieve the desired threshold t,\n",
    "            # be to the desired total LSH size n?\n",
    "            err = abs(n - b * r)\n",
    "            \n",
    "            if err < minerr:\n",
    "                best = r\n",
    "                minerr = err\n",
    "                \n",
    "        return best\n",
    "\n",
    "    def hash(self, seq):\n",
    "        \"\"\"\n",
    "        Generate one locality-sensitive hash ID for each band,\n",
    "        from the input sequence `seq` (tokens, n-grams, etc.).\n",
    "        \"\"\"\n",
    "        sig = self.min_hasher.sign(seq)\n",
    "        \n",
    "        for band in zip(*(iter(sig),) * self.bandwidth):\n",
    "            # the \"salt\" is to make this approach more robust\n",
    "            # and is specific to how hashing works (out of scope here)\n",
    "            # this final hash is only used to only return \n",
    "            # one unique bucket it per band;\n",
    "            # the `band` itself is a list of `bandwith`\n",
    "            # integers from the signature\n",
    "            yield hash(\"salt\" + str(band) + \"tlas\")\n",
    "\n",
    "    @property\n",
    "    def exact_threshold(self):\n",
    "        \"\"\"\n",
    "        The actual threshold,\n",
    "        as defined by the calculated bandwith.\n",
    "        \"\"\"\n",
    "        \n",
    "        r = self.bandwidth\n",
    "        b = self.size / r\n",
    "        return (1. / b) ** (1. / r)\n",
    "\n",
    "    @property\n",
    "    def n_bands(self):\n",
    "        \"\"\"\n",
    "        Calculate the number of bands.\n",
    "        \"\"\"\n",
    "        \n",
    "        return int(self.size / self.bandwidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zip-list iterated over in BandedLSH.hash is simply to generate the signature hash ID subset for each band. E.g., assuming size = 6 for the BandedLSH and, therefore, also for the min-hash signature, and assuming the chosen bandwidth would be, say, 2 (from BandedLSH.get_bandwidth), the cryptical iterator zip(*(iter(sig),) * self.bandwidth) above would produce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (5, 6)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature = [1,2,3,4,5,6]\n",
    "bandwidth = 2\n",
    "list(zip(*(iter(signature),) * bandwidth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, what we get out of BandedLSH.hash is one hash ID value for each band given an item's (min-hash) signature size and Jaccard simlilarty threshold target, by calculating the best bandwidth given those two input parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bandwidth calculated: 2\n",
      "resulting n. bands: 150\n",
      "exact similarity threshold: 0.08164965809277261\n"
     ]
    }
   ],
   "source": [
    "hasher = BandedLSH(300, 0.1)\n",
    "print(\"bandwidth calculated:\", hasher.bandwidth)\n",
    "print(\"resulting n. bands:\", hasher.n_bands)\n",
    "print(\"exact similarity threshold:\", hasher.exact_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to bandwidth (and the number of bands, too). This indicates how \"hard\" the threshold can be cut off. If the bandwidth is very small and the bands very large, you might rather fuzzy clusters (and a very streched sigmoid in the plot below). The bandwidth (rows/band) is proportional to the (desired) Jaccard similiarty (the horizontal position of the sigmoid plotted below).\n",
    "\n",
    "To visualize the \"cutoff hardness\" of your setup, you can visualize the probability of two documents being added to the same cluster (i.e., set of buckets) as a function of their Jaccard similarity. (see slide \"Banded Locaility Sensitive Minhashing\", $p_{agreement}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110d452e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bands = 150\n",
    "bandwidth = 2\n",
    "p_agreement = lambda sim: 1 - (1 - sim**bandwidth)**bands\n",
    "x = np.arange(0.0, 0.25, 0.01)\n",
    "\n",
    "plt.plot(x, p_agreement(x))\n",
    "plt.title('Clustering probability')\n",
    "# (as a function of bandwidth and n. bands)\n",
    "plt.xlabel('Jaccard similarity')\n",
    "plt.ylabel('Clustering P')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with the above cell, chaning `bandwidth` and `bands` to understand how those two parameters influnce the likelihood of two documents sharing *at least one bucket* given their Jaccard similarity.\n",
    "\n",
    "## Document indexing\n",
    "\n",
    "While the toy implementation above was practical to discuss the algorithm, we will be using a special [LSH **Forest**](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LSHForest.html#sklearn-neighbors-lshforest) implementation to retrieve the closest matching document given a query (document).\n",
    "\n",
    "![title](img/lsh_forest.png)\n",
    "\n",
    "Another attractive LSH implemention is avaliable from the [`datasketch`](https://ekzhu.github.io/datasketch/lsh.html) Python library.\n",
    "\n",
    "### Corpus preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus sizes: train = 11314 ; test = 7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from segtok.segmenter import split_multi\n",
    "from segtok.tokenizer import word_tokenizer, split_contractions\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "train, test = fetch_20newsgroups(), fetch_20newsgroups(subset='test')\n",
    "\n",
    "stopwords_en = frozenset(stopwords.words('english'))\n",
    "stopwords_none = frozenset([])\n",
    "\n",
    "\n",
    "def tokenize(raw_text):\n",
    "    \"\"\"\n",
    "    Convert text to lower-case tokens of length > 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    for sentence in split_multi(raw_text):\n",
    "        for raw in split_contractions(word_tokenizer(sentence)):\n",
    "            token = raw.lower()\n",
    "            \n",
    "            if len(token) > 2 and any(c.isalnum() for c in token):\n",
    "                yield token\n",
    "\n",
    "                \n",
    "def filter_and_stem(raw_text, stopwords):\n",
    "    \n",
    "    for token in tokenize(raw_text):\n",
    "        yield token\n",
    "        if token not in stopwords:\n",
    "            yield stemmer.stem(token)\n",
    "        else:\n",
    "            yield None\n",
    "                \n",
    "                \n",
    "def shingle(\n",
    "    raw_text,\n",
    "    stopwords=stopwords_none,\n",
    "    bigrams=False):\n",
    "    \"\"\"\n",
    "    Shingle text to stop-word-free,\n",
    "    stemmed uni- and bi-grams.\n",
    "    \"\"\"\n",
    "    \n",
    "    last = None\n",
    "    \n",
    "    for t in filter_and_stem(raw_text, stopwords):\n",
    "        if t is not None:\n",
    "            yield t\n",
    "            \n",
    "            if bigrams and last is not None:\n",
    "                yield \"{}_{}\".format(last, t)\n",
    "            \n",
    "        last = t\n",
    "\n",
    "print(\"Corpus sizes: train =\",\n",
    "      len(train.data),\n",
    "      \"; test =\", len(test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how that tokenization/shingling works on a document in our collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n"
     ]
    }
   ],
   "source": [
    "example = train['data'][0].strip()\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real-world scenario, you'd probably start cleaning up the above \"mess\", removing the header and possibly the footers from the messages.\n",
    "\n",
    "You can either use `tokenize` to work with single tokens, or `shingle` to work with [higher] n-grams and stopword-filtering, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from saw car looked funky separate whatev line organ there park tellm thank engine wondering nntp-posting-host rac3.wam.umd.edu can neighborhood 2-door thanks what made body model rest door your 70s the history university really look 60s lines thing know please e-mail univers thi bodi wonder anyone all histori sport front info engin organization separ anyon 60 out were 70 day specs years subject addition production funki this year addit colleg other doors bumper product wam.umd.edu whatever late wa earli sports realli pleas could bricklin college enlighten small where have lerxst looking early called brought was name call spec maryland you tellme\n"
     ]
    }
   ],
   "source": [
    "tokens = frozenset(shingle(example))\n",
    "print(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the whole 20News *train* corpus into tokens - this can take a while (possibly 5 minutes!) on most machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 7s, sys: 668 ms, total: 2min 7s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%time corpus = [frozenset(shingle(doc)) for doc in train['data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity threshold estimation\n",
    "\n",
    "First, we'd like to find out how our document similarity behaves across our corpus.\n",
    "To do that we select a random \"target\" document and compute it's Jaccard similiary against all other documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other doc with max. Jaccard similarity = 0.3722627737226277\n",
      "Mean Jaccard similarity = 0.0795601381685\n",
      "Median Jaccard similarity = 0.07936507936507936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11ccd0780>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b36d860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "target = random.choice(range(len(corpus)))\n",
    "others = [i for i in range(len(corpus)) if i != target]\n",
    "jsim = lambda i: jaccard(corpus[target], corpus[i])\n",
    "max_sim = max((jsim(i) for i in others))\n",
    "\n",
    "print(\"other doc with max. Jaccard similarity =\",\n",
    "      max_sim)\n",
    "\n",
    "sims = pd.Series(jsim(i) for i in others)\n",
    "print(\"Mean Jaccard similarity =\", sims.mean())\n",
    "print(\"Median Jaccard similarity =\", sims.median())\n",
    "sims.plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the above cell a few times to get a feeling for the similarty.\n",
    "\n",
    "What you can see is that \"plain\" LSH will *not* be very useful for **clustering** our corpus (represented as _bags-of-words_), because most documents have a very low similiarty to each other.\n",
    "Rather, where LSH shines is detecting duplicates or even plagiarism, because then the Jaccard simliarity is (very) high.\n",
    "\n",
    "### LSH Forest setup\n",
    "\n",
    "A special variant of LSH, LSH *Forest*, is able to detect the best match for a query (document) if there is any reasonable Jaccard similarity between that query and some document in your collection.\n",
    "In addition, and as before for the document classification, we set up a TF-IDF document vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.72 s, sys: 163 ms, total: 5.88 s\n",
      "Wall time: 5.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neighbors import LSHForest\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    min_df=5,\n",
    "    max_df=0.5,\n",
    "    #ngram_range=(1,2),\n",
    "    token_pattern='(?u)(?:\\\\b|(?<=_))[^\\W_][^\\W_]+(?:\\\\b|(?=_))',\n",
    ")\n",
    "# train (aka, fit) the vectorizer to the train data:\n",
    "X_train = vectorizer.fit_transform(train.data)\n",
    "\n",
    "classifier = LSHForest(n_estimators=30, n_neighbors=1000)\n",
    "classifier.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we've indexed all our documents in virtually no time! Please let this sink in for a moment - most search engines spend ages indexing content, while this solution hardly spends any noticable time at all (compared to the collection size, over 11,000 documents). This property of LSH is *extremely* useful when working with document streams where you don't have much time to process each document!\n",
    "\n",
    "The parameter that has the strongest impact on indexing *and, particularly,* search times is `n_estimators`, but increasing it improves the retrieval quality. Depending on your needs, you will almost certainly want to adjust that parameter to your use-case. The `n_neighbors` parameter only sets the default number of documents returned by a query, but makes the queries  not noticably slower. For the reamining parameters, please refer to the [SciKit-Learn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LSHForest.html#sklearn.neighbors.LSHForest).\n",
    "\n",
    "## Document search\n",
    "\n",
    "Now, lets use this classifier to predict the nearest neighbor-ranked list for each of the \"test\" documents that we did not yet index (so this isn't just trivially returning exact matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 53s, sys: 4.72 s, total: 1min 58s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# with the fitted vectorizer, transform the test data:\n",
    "X_test = vectorizer.transform(test.data)\n",
    "\n",
    "# now run the queries:\n",
    "indices = classifier.kneighbors(\n",
    "    X_test,\n",
    "    n_neighbors=1000,\n",
    "    return_distance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is quite noteworthy here is the time it took to search our documents. We searched for around 7,000 documents in a pool of almost 12,000; So each query just took a small fraction of a second!\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Now we evaluate how good that index mechanism is.\n",
    "We will query the index with our test documents, only retrieving the best match from the indexed training documents.\n",
    "Then, we check if that retrieved training document indeed would have been the most similar document for that query document from the test set (in terms of Jaccard similarity).\n",
    "\n",
    "That is, we will evaluate the (mean) *true* rank of the returned document from LSH Forest by ordering all *training* documents against the query document using the Jaccard similarity measure.\n",
    "\n",
    "First, generate the list of the best matching training documents for all test documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best prediction true rank =     2 with sim = 0.169 ; #1 doc sim = 0.169\n",
      "best prediction true rank =     2 with sim = 0.223 ; #1 doc sim = 0.368\n",
      "best prediction true rank =  2054 with sim = 0.104 ; #1 doc sim = 0.191\n",
      "best prediction true rank =     1 with sim = 0.274 ; #1 doc sim = 0.274\n",
      "best prediction true rank =   557 with sim = 0.103 ; #1 doc sim = 0.154\n",
      "best prediction true rank =     1 with sim = 0.352 ; #1 doc sim = 0.352\n",
      "best prediction true rank =     1 with sim = 0.214 ; #1 doc sim = 0.214\n",
      "best prediction true rank =     5 with sim = 0.149 ; #1 doc sim = 0.161\n",
      "best prediction true rank =   559 with sim = 0.086 ; #1 doc sim = 0.137\n",
      "best prediction true rank =   222 with sim = 0.108 ; #1 doc sim = 0.145\n",
      "best prediction true rank =     6 with sim = 0.178 ; #1 doc sim = 0.214\n",
      "best prediction true rank =    38 with sim = 0.138 ; #1 doc sim = 0.188\n",
      "best prediction true rank =     1 with sim = 0.975 ; #1 doc sim = 0.975\n",
      "best prediction true rank =     1 with sim = 0.183 ; #1 doc sim = 0.183\n",
      "\n",
      "mean reciprocal rank = 0.362\n",
      "median rank = 6\n",
      "\n",
      "CPU times: user 24.8 s, sys: 111 ms, total: 24.9 s\n",
      "Wall time: 24.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from statistics import median\n",
    "\n",
    "trr = 0. # total reciprocal ranks\n",
    "ranks = []\n",
    "n_queries = len(test['data']) // 100 # do 1% only\n",
    "samples = random.sample(list(enumerate(indices)), n_queries)\n",
    "\n",
    "for i, predictions in samples:\n",
    "    # query document from\n",
    "    query = frozenset(shingle(test['data'][i]))\n",
    "    prediction = predictions[0]\n",
    "    \n",
    "    # compare the query doc to each doc in training set\n",
    "    # this is the most costly part\n",
    "    pairs = [(jaccard(doc, query), idx)\n",
    "             for idx, doc in enumerate(corpus)]\n",
    "    \n",
    "    # sort and reverse the results (also costly, but much less)\n",
    "    pairs = list(reversed(sorted(pairs)))\n",
    "    \n",
    "    # finally, generate and print some statistics for evaluation\n",
    "    max_similarity = pairs[0][0]\n",
    "    _, correct_ranking = zip(*pairs)\n",
    "    rank = correct_ranking.index(prediction) + 1\n",
    "    sim = 0.0\n",
    "    \n",
    "    if rank == 0: # rank last if nothing found\n",
    "        rank = len(corpus)\n",
    "    else: # get the Jaccard similarity for the matched doc \n",
    "        sim = pairs[rank-1][0]\n",
    "\n",
    "    trr += 1 / rank\n",
    "    ranks.append(rank)\n",
    "        \n",
    "    # report a few cases\n",
    "    if random.random() < 0.2:\n",
    "        print(\"best prediction true rank = %5d\" % rank,\n",
    "              \"with sim = %.3f\" % sim,\n",
    "              \"; #1 doc sim = %.3f\" % max_similarity)\n",
    "\n",
    "mrr = trr / n_queries # mean reciprocal rank\n",
    "print(\"\\nmean reciprocal rank = %.3f\" % mrr)\n",
    "print(\"median rank = %d\\n\" % median(ranks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this process is around one to two orders of magnitude slower (notice that we queried 100 times less documents than before against LSH Forest, but that fraction about the same time to run). Notice that pairwise lookups scale with the number of documents in the index, while LSH Forest (mostly) just scales with the number of trees (`n_estimators`).\n",
    "\n",
    "Another useful insight is that when the predicted document isn't the first, the overall best document (\"`true doc: sim.`\") tends to be of relatively low similarity, too. So this is maybe not perfect, but at least it does not fail for the \"obvious\" cases (with high similarities), only for the less clear-cut matches.\n",
    "\n",
    "Therefore, let's look at the other important evaluation scenario: How many documents, on average, would this LSH Forest have to report to cover the truly best match - letting us estimate how big the sets we want to retrieve should be (and the number of pairwise comparisons needed to rank that set against the query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reciprocal rank = 0.794\n",
      "median rank = 1\n",
      "cache misses = 47\n",
      "\n",
      "CPU times: user 20.3 s, sys: 65.2 ms, total: 20.4 s\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "trr = 0.0 # total reciprocal ranks\n",
    "ranks = []\n",
    "misses = 0\n",
    "n_queries = len(test['data']) // 100 # do 1% only\n",
    "samples = random.sample(list(enumerate(indices)), n_queries)\n",
    "\n",
    "for i, predictions in samples:\n",
    "    query = frozenset(shingle(test['data'][i]))\n",
    "    \n",
    "    # compare the query doc to each doc in training set\n",
    "    pairs = [(jaccard(doc, query), idx)\n",
    "             for idx, doc in enumerate(corpus)]\n",
    "    \n",
    "    # reverse-sort the results by Jaccard similarity\n",
    "    pairs = list(reversed(sorted(pairs)))\n",
    "    \n",
    "    # finally, generate and print some statistics for evaluation\n",
    "    best_doc = pairs[0][1]\n",
    "    \n",
    "    try:\n",
    "        rank = np.where(predictions==best_doc)[0][0] + 1\n",
    "        trr += 1 / rank\n",
    "        ranks.append(rank)\n",
    "    except IndexError:\n",
    "        misses += 1\n",
    "\n",
    "mrr = trr / (n_queries - misses) # mean reciprocal rank\n",
    "print(\"mean reciprocal rank = %.3f\" % mrr)\n",
    "print(\"median rank = %d\" % median(ranks))\n",
    "# n. times the top 1000 did not contain the best match at all:\n",
    "print(\"cache misses = %d\\n\" % misses) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is a bit more clarifying. So we do have about half of our cases where we don't find the top match at all. I.e., our LSH Forest returns a set of good matches, but it does not necessarily return the best match. So depending on your use-case beware of this.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "You can build very efficient and very simple document and query search engines with LSH Forest and LSH Ensemble, but the overall performance only gives you a \"sketchy\" view of the true result, i.e., with holes. Admittedly, document clustering with LSH is a bit of a black art. But then, about any data sketching is mostly black art... However, getting this right is extremely rewarding, because indexing documents using only (tree) hashes is probably the most performant and scalable path towards building a super-fast algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
