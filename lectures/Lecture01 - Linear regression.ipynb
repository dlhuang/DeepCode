{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data like this, how can we learn to predict the prices of other houses in Portland, as a function of the size of their living areas?\n",
    "To establish notation for future use, we’ll use $x^{(i)}$ to denote the “input”\n",
    "variables (living area in this example), also called input features, and $y^{(i)}$\n",
    "to denote the “output” or target variable that we are trying to predict\n",
    "(price). A pair $(x^{(i)} , y^{(i)} )$ is called a training example, and the dataset\n",
    "that we’ll be using to learn—a list of m training examples ${(x^{(i)} , y^{(i)} ); i =\n",
    "1, . . . , m}$—is called a training set. Note that the superscript “(i)” in the\n",
    "notation is simply an index into the training set, and has nothing to do with\n",
    "exponentiation. We will also use $X$ denote the space of input values, and $Y$\n",
    "the space of output values.\n",
    "\n",
    "To describe the supervised learning problem slightly more formally, our\n",
    "goal is, given a training set, to learn a function $h:X\\in Y$ so that $h(x)$ is a\n",
    "“good” predictor for the corresponding value of y. For historical reasons, this\n",
    "function h is called a hypothesis. Seen pictorially, the process is therefore\n",
    "like this:\n",
    "<img src='../images/hypothesis.png' width='300' alt=''>\n",
    "\n",
    "When the target variable that we’re trying to predict is continuous, such\n",
    "as in our housing example, we call the learning problem a regression prob-\n",
    "lem. When y can take on only a small number of discrete values (such as\n",
    "if, given the living area, we wanted to predict if a dwelling is a house or an\n",
    "apartment, say), we call it a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform supervised learning, we must decide how we’re going to rep-\n",
    "resent functions/hypotheses h in a computer. As an initial choice, let’s say\n",
    "we decide to approximate $y$ as a linear function of $x$:\n",
    "\n",
    "$$h_{\\theta} = \\theta_0 + \\theta_1x$$\n",
    "\n",
    "Here, the $θ_i’s$ are the parameters (also called weights) parameterizing the\n",
    "space of linear functions mapping from $X$ to $Y$. When there is no risk of\n",
    "confusion, we will drop the $\\theta$ subscript in $h_{\\theta}(x)$, and write it more simply as\n",
    "$h(x)$. To simplify our notation, we also introduce the convention of letting\n",
    "$x_0 = 1$ (this is the intercept term), so that\n",
    "\n",
    "$$h(x) = \\sum_{i=1}^m \\theta_ix_i = \\theta^Tx$$\n",
    "\n",
    "where on the right-hand side above we are viewing $\\theta$ and $x$ both as vectors,\n",
    "and here $n$ is the number of input variables (not counting $x_0$). Now, given a training set, how do we pick, or learn, the parameters $\\theta$? One reasonable method seems to be to make $h(x)$ close to $y$, at least for the training examples we have. To formalize this, we will define a function that measures, for each value of the $θ’s$, how close the $h(x^{(i)})$'s are to the corresponding $y^{(i)}$'s. We define the cost or loss function $L(h({x}),y)$, is some measure of prediction error. There are many types of loss functions.\n",
    "* L1-norm: $L(h({x}),y) = \\|h({x}) - y\\|_1$ -- city block distance -- lasso\n",
    "* L2-norm: $L(h({x}),y) = \\|h({x}) - y\\|_2$ -- just the euclidian -- ridge\n",
    "* p-norm: $L(h({x}),y) = \\|h({x}) - y\\|_p$ -- more generally speaking\n",
    "* Square-error: $L(h({x}),y) = \\|h({x}) - y\\|_2^2$ -- used in linear regression and logistic regression\n",
    "* Hinge-loss: $L(h({x}),y) = \\text{max}(0,1-yh({x}))$ -- used in SVMs for classification.\n",
    "* Exponential-loss: $L(h({x}),y) = e^{-yh({x})}$ -- used in adaboost for classification\n",
    "\n",
    "The loss function is equal to the negated conditional log-likelihood\n",
    "\\begin{align*}\n",
    "L(h({x}),y) &= - \\log p(\\mathcal{D}|{w})\\\\\n",
    "&= - \\sum_m \\log p(y_m|{x}_m,{w})\n",
    "\\end{align*}\n",
    "\n",
    "Given some data, we can approximate the expected risk with the empirical risk given by\n",
    "\\begin{align*}\n",
    "R_{\\mathcal{D}}(h) &= \\frac{1}{m} \\sum_m L(h({x}_m),y_m)\n",
    "\\end{align*}\n",
    "Also, with infinite data, empirical risk is the expected risk\n",
    "\\begin{align*}\n",
    "\\lim_{m \\to \\infty} R_{\\mathcal{D}}(h) &= R(h)\n",
    "\\end{align*}\n",
    "\n",
    "This gives us an empirical risk (or cross-entropy) of\n",
    "\\begin{align*}\n",
    "R_{\\mathcal{D}}(h) = \\frac{1}{m} \\sum_m \\|y_m - h({x})\\|_2^2\n",
    "\\end{align*}\n",
    "In this case, empirical risk is often referred to as the residual-sum-of-squares (RSS) or mean-square-error (MSE). The cost function used here defined as:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "We want to choose $\\theta$ so as to minimize $J(\\theta)$. To do so, let’s use a search\n",
    "algorithm that starts with some “initial guess” for $\\theta$, and that repeatedly\n",
    "changes $\\theta$ to make $J(\\theta)$ smaller, until hopefully we converge to a value of\n",
    "$\\theta$ that minimizes $J(\\theta)$. Specifically, let’s consider the gradient descent\n",
    "algorithm, which starts with some initial $\\theta$, and repeatedly performs the\n",
    "update:\n",
    "\n",
    "$$\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial{\\theta_{j}}}J(\\theta)$$\n",
    "\n",
    "\n",
    "(This update is simultaneously performed for all values of $j = 0, . . . , n.$)\n",
    "Here, $\\alpha$ is called the learning rate. This is a very natural algorithm that\n",
    "repeatedly takes a step in the direction of steepest decrease of $J$.\n",
    "In order to implement this algorithm, we have to work out what is the\n",
    "partial derivative term on the right hand side. Let’s first work it out for the\n",
    "case of if we have only one training example $(x, y)$, so that we can neglect\n",
    "the sum in the definition of $J$. We have:\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial{\\theta_{j}}}J(\\theta) &= \\frac{\\partial}{\\partial{\\theta_{j}}}\\frac{1}{2}(h_{\\theta}(x)-y)^2\\\\\n",
    "&= 2.\\frac{1}{2}(h_{\\theta}(x)-y)\\frac{\\partial}{\\partial{\\theta_{j}}}(h_{\\theta}(x)-y)\\\\\n",
    "&= (h_{\\theta}(x)-y)\\frac{\\partial}{\\partial{\\theta_{j}}}(\\sum_{i=0}^{n}\\theta_i x_i-y)\\\\\n",
    "&= (h_{\\theta}(x)-y)x_j\n",
    "\\end{align}\n",
    "\n",
    "Using the rule for a single training example, we derive the gradient descent algorithm for linear regression:\n",
    "\n",
    "\\begin{align}\n",
    "repeat \\{\\\\\n",
    "&\\theta_j = \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)}).x^{(i)}_j\\\\\n",
    "\\}\n",
    "\\end{align}\n",
    "\n",
    "Simultaneously update all values of $j = 0, . . . , n.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "One problem that frequently occures in linear regression is overfitting. The standard solution for previnting overfitting is regularization. Since we know that the empirical risk will underestimate the true risk, add a penalty to complex functions $h$ to try to compensate for this. Then, instead of doing\n",
    "$$\n",
    "min_h R_D(h) = E\\big[L(h({x}_n),y_n)\\big]\n",
    "$$\n",
    "\n",
    "we can do\n",
    "$$\n",
    "min_h R_D(h) + \\lambda h(\\theta)\n",
    "$$\n",
    "Notice that we can rephrase this unconstrained linear optimization as an constrained optimization. For any given $\\lambda$ there is a $c$ such that the same solution can be obtained from\n",
    "$$\n",
    "min_f R_D(h) \\\\\n",
    "s.t.\\;\\;\\;h(\\theta) \\le c\n",
    "$$\n",
    "\n",
    "**The Lasso estimator** is useful to impose sparsity on the coefficients. In other words, it is to be prefered if we believe that many of the features are not relevant.\n",
    "\n",
    "$$\\hat{\\theta}_{lasso} = \\text{argmin}_{\\theta}\\left\\{\\frac{1}{2}\\sum_{i=1}^N (y_i - \\sum_{j=0}^k x_{ij} \\theta_j)^2 + \\lambda \\sum_{j=1}^k |\\theta_j| \\right\\}$$\n",
    "\n",
    "The **ridge estimator** is a simple, computationally efficient regularization for linear regression.\n",
    "\n",
    "$$\\hat{\\theta}_{ridge} = \\text{argmin}_{\\theta}\\left\\{\\sum_{i=1}^N (y_i - \\sum_{j=0}^k x_{ij} \\theta_j)^2 + \\lambda \\sum_{j=1}^k \\theta_j^2 \\right\\}$$\n",
    "\n",
    "**ElasticNet** is a compromise between lasso and ridge regression.\n",
    "\n",
    "$$\\hat{\\theta}_{elastic} = \\text{argmin}_{\\theta}\\left\\{\\frac{1}{2}\\sum_{i=1}^N (y_i - \\sum_{j=0}^k x_{ij} \\theta_j)^2 + (1 - \\alpha) \\sum_{j=1}^k \\theta^2_j + \\alpha \\sum_{j=1}^k |\\theta_j| \\right\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal equation\n",
    "Method of solve for $\\theta$ analytically. Intuition: Given a training set ${(x^{(i)} , y^{(i)} ); i =\n",
    "1, . . . , m}$, We know the cost function for linear regression is\n",
    "    \n",
    "$$J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2 = \\frac{1}{2}(X\\theta-\\hat{y})^T(X\\theta-\\hat{y})$$\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta}J(\\theta) &= \\nabla_{\\theta} \\frac{1}{2}(X\\theta-\\hat{y})^T(X\\theta-\\hat{y})\\\\\n",
    "&= \\frac{1}{2} \\nabla_{\\theta} (\\theta^T X^T X\\theta - \\theta^TX^T\\hat{y} - \\hat{y}^TX\\theta + \\hat{y}^T\\hat{y}) \\\\&= \\frac{1}{2} \\nabla_{\\theta} \\;{tr}(\\theta^T X^T X\\theta - \\theta^TX^T\\hat{y} - \\hat{y}X\\theta + \\hat{y}^T\\hat{y}) \\\\\n",
    "&= \\frac{1}{2} \\nabla_{\\theta} ({tr}(\\theta^T X^T X\\theta) - 2{tr}(\\hat{y}^T X\\theta)) \\\\\n",
    "&= \\frac{1}{2} \\nabla_{\\theta} (X^TX\\theta + X^TX\\theta - 2 X^T\\hat{y}) \\\\\n",
    "&= (X^TX\\theta - X^T\\hat{y}) \\\\\n",
    "0 &= (X^TX\\theta - X^T\\hat{y}) \\\\\n",
    "X^TX\\theta &= X^T\\hat{y}\\\\\n",
    "\\therefore \\theta &= (X^TX)^{-1}X^T\\hat{y}\n",
    "\\end{align*}\n",
    "\n",
    "The value of $\\theta$ that minimize $J(\\theta)$ is given by the closed form equation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1\n",
       "0  6.1101  17.5920\n",
       "1  5.5277   9.1302\n",
       "2  8.5186  13.6620\n",
       "3  7.0032  11.8540\n",
       "4  5.8598   6.8233"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../exercise/ex1/data/ex1data1.txt', sep=\",\", header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data[0], data[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
    "#### Z-score normalization\n",
    "The result of **standardization** (or **Z-score normalization**) is that the features will be rescaled so that they'll have the properties of a standard normal distribution with $\\mu = 0$ and $\\sigma = 1$, where $\\mu$ is the mean (average) and $\\sigma$ is the standard deviation from the mean; standard scores (also called ***z*** scores) of the samples are calculated as follows:\n",
    "\n",
    "\\begin{equation} z = \\frac{x - \\mu}{\\sigma}\\end{equation} \n",
    "\n",
    "#### Min-Max scaling\n",
    "Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms. An alternative approach to Z-score normalization (or standardization) is the so-called **Min-Max scaling** (often also simply called \"normalization\" - a common cause for ambiguities). In this approach, the data is scaled to a fixed range - usually 0 to 1. The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.\n",
    "\n",
    "A Min-Max scaling is typically done via the following equation:\n",
    "\n",
    "\\begin{equation} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def z_score_normalize(X):\n",
    "    X = (X - X.mean()) / X.std()\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "h_{\\theta} = y_{predicted} = {w}^T{x}\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (y_i - y_{predicted})^2\n",
    "\\end{align*}\n",
    "Here, $\\theta=w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "d_norm = z_score_normalize(data)\n",
    "train_X = d_norm[0]\n",
    "train_Y = d_norm[1]\n",
    "\n",
    "n_samples = data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, name= 'X')\n",
    "Y = tf.placeholder(tf.float32, name= 'Y')\n",
    "\n",
    "W = tf.Variable(np.random.normal(), name='weight')\n",
    "b = tf.Variable(np.random.normal(), name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\theta_j = \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)}).x^{(i)}_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.add(tf.multiply(X, W), b)\n",
    "\n",
    "cost = tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * n_samples)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning cost= 0.19458192586898804, weight= 0.5376732349395752, bias= 0.07124785333871841\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(optimizer, feed_dict={X: train_X, Y: train_Y})\n",
    "    \n",
    "    traning_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
    "    weight = sess.run(W)\n",
    "    bias = sess.run(b)\n",
    "    print('Traning cost= {}, weight= {}, bias= {}'.format(traning_cost, weight, bias))\n",
    "    \n",
    "    plt.scatter(train_X, train_Y, label='Normalized data')\n",
    "    plt.plot(train_X, weight * train_X + bias, label='Fitted line')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
