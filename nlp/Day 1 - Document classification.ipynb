{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document classification\n",
    "\n",
    "Practical course material for the ASDM Class 09 (Text Mining) by Florian Leitner.\n",
    "\n",
    "© 2017 Florian Leitner. All rights reserved.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Today's lab will cover the two most fundamental techniques of Text Mining: Document classification and indexing; This notebook covers the former.\n",
    "\n",
    "First we run the standard setup seen in the preparatory notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline --no-import-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have **SciKit-Learn** installed or install it with:\n",
    "\n",
    "- Anaconda Python: `conda install sklearn`\n",
    "- Stock Python: `pip3 install sklearn`\n",
    "\n",
    "Once installed, the following import should work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus setup\n",
    "\n",
    "The de facto standard evaluation dataset of Text Mining (analgous to the  [MINST \"database\" of handwritten digits](http://yann.lecun.com/exdb/mnist/) for computer vision) is the [20 Newsgroups dataset](http://qwone.com/~jason/20Newsgroups/) (from 1997 - yes, its vocabulary is outdated...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fetch_20newsgroups in module sklearn.datasets.twenty_newsgroups:\n",
      "\n",
      "fetch_20newsgroups(data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True)\n",
      "    Load the filenames and data from the 20 newsgroups dataset.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <20newsgroups>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    subset : 'train' or 'test', 'all', optional\n",
      "        Select the dataset to load: 'train' for the training set, 'test'\n",
      "        for the test set, 'all' for both, with shuffled ordering.\n",
      "    \n",
      "    data_home : optional, default: None\n",
      "        Specify a download and cache folder for the datasets. If None,\n",
      "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "    \n",
      "    categories : None or collection of string or unicode\n",
      "        If None (default), load all the categories.\n",
      "        If not None, list of category names to load (other categories\n",
      "        ignored).\n",
      "    \n",
      "    shuffle : bool, optional\n",
      "        Whether or not to shuffle the data: might be important for models that\n",
      "        make the assumption that the samples are independent and identically\n",
      "        distributed (i.i.d.), such as stochastic gradient descent.\n",
      "    \n",
      "    random_state : numpy random number generator or seed integer\n",
      "        Used to shuffle the dataset.\n",
      "    \n",
      "    download_if_missing : optional, True by default\n",
      "        If False, raise an IOError if the data is not locally available\n",
      "        instead of trying to download the data from the source site.\n",
      "    \n",
      "    remove : tuple\n",
      "        May contain any subset of ('headers', 'footers', 'quotes'). Each of\n",
      "        these are kinds of text that will be detected and removed from the\n",
      "        newsgroup posts, preventing classifiers from overfitting on\n",
      "        metadata.\n",
      "    \n",
      "        'headers' removes newsgroup headers, 'footers' removes blocks at the\n",
      "        ends of posts that look like signatures, and 'quotes' removes lines\n",
      "        that appear to be quoting another post.\n",
      "    \n",
      "        'headers' follows an exact standard; the other filters are not always\n",
      "        correct.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "help(fetch_20newsgroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and perpare the dataset (needs a working internet connection):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups()\n",
    "test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display some information about our corpus: n. documents, corpus size (characters), n. labels (newsgroups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  11314 documents with 22.1 million UTF-16 characters\n",
      "Test set    :  7532 documents with 13.8 million UTF-16 characters\n",
      "\n",
      "20 Newsgroups:\n",
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "def nmmchars(docs):\n",
    "    return sum(map(len, docs)) / 1e6\n",
    "\n",
    "print(\"Training set:\",\n",
    "      \" %5d documents with %4.1f million UTF-16 characters\" % (\n",
    "    len(train.data), nmmchars(train.data)))\n",
    "print(\"Test set    :\", \n",
    "      \"%5d documents with %4.1f million UTF-16 characters\" % (\n",
    "    len(test.data), nmmchars(test.data)))\n",
    "\n",
    "print(\"\\n%d Newsgroups:\" % len(train.target_names))\n",
    "print('\\n'.join(train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example document (NB: *always* **look at your data** - I could not emphasise this more!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n"
     ]
    }
   ],
   "source": [
    "print(train.data[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the label vectors $y_{train}$ and  $y_{test}$ for the training and test set (as integers referencing the newsgroups):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 4 4 ..., 3 1 8]\n",
      "[ 7  5  0 ...,  9  6 15]\n"
     ]
    }
   ],
   "source": [
    "y_train, y_test = train.target, test.target\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to using SciKit-Learn, we get *everything* we discussed in the lecture (and some more) in one nice package: \n",
    "\n",
    "- A simple$^§$ regex-based tokenization: `(?u)\\\\b\\\\w\\\\w+\\\\b`,\n",
    "- English stop-word filtering (other langauges: provide your own list), \n",
    "- Lower-case text normalization,\n",
    "- Vocabulary selection (limit the vocabulary to a predefined set),\n",
    "- Word n-gram (\"term\") generation (**default: none**!),\n",
    "- Inverted (term) indexing, and\n",
    "- TF-IDF term weighting\n",
    "\n",
    "from the **[`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)**. In fact, this class even provides several niceties we are not using: character decoding, removal of accents, and even *character* n-gram feature generation (as opposed to token (\"word\") n-grams).\n",
    "\n",
    "$^§$ You might want to replace the default regex with: `(?u)(?:\\\\b|(?<=_))[^\\W_][^\\W_]+(?:\\\\b|(?=_))` to avoid having underscores in words (except maybe when analyzing Twitter tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module sklearn.feature_extraction.text:\n",
      "\n",
      "__init__(self, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      "    Initialize self.  See help(type(self)) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "help(TfidfVectorizer.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF parameters\n",
    "\n",
    "The parameters that control TF-IDF term weighting are:\n",
    "\n",
    "#### `binary`, Boolean, `True`\n",
    "\n",
    "Only 0/1 counts for words: aka, in document or not.\n",
    "Despite possible doubts, this often works surprisingly well, because it is a strong *regularization technique*.\n",
    "\n",
    "#### `max_df`, $(0.0,1.0]$, `1.0`\n",
    "\n",
    "Remove words in more documents than the `max_df` proportion.\n",
    "Think: \"learning corpus-specific, automatic stop-word filtering\";\n",
    "If your corpus is large enough (thousands or more documents), use this feature instead of brewing up your own stop-word list!\n",
    "Sometimes even including a stop-word list might lead you to filter out some of your best features.\n",
    "But as always in machine learning: The best strategy is to evaluate all approaches (i.e., with and without stop-words and/or DF filtering).\n",
    "Good values are in the range of $[0.25, 0.75]$, depending on corpus size.\n",
    "\n",
    "#### `min_df`, $[1,∞)$, `1`\n",
    "\n",
    "Filter (remove) words that appear in less than `min_df` documents.\n",
    "This again is regularization technique and can help remove noise.\n",
    "A good default value for this option is in the range of $[3,5]$, depending on corpus size (and possibly even `2` or even `10` for extreme sizes).\n",
    "\n",
    "#### `use_idf`, Boolean, `True`\n",
    "\n",
    "Dis- or enable IDF re-weighting (possibly only using term frequency).\n",
    "This option is particularly useful if you want to apply TF scaling (only) and/or other functionality of this vectorizer for some other means (than document classificiation/clustering).\n",
    "Also, for a **small training set**, IDF scaling might introduce more noise than anything else, in which case you also want to disable IDF.\n",
    "\n",
    "#### `smooth_idf`, Boolean, `True`\n",
    "\n",
    "If using IDF, add Laplace (aka. lidstone) +1 smoothing to all document frequency counts, to avoid divide-by-zero errors (i.e., assume each word was seen at least once at all times).\n",
    "The default is `True`, but you can use `False` if you then limit your (*test set*) vocabulary (i.e., \"for prediction\") to the words seen during training;\n",
    "I.e., if you have a **small dataset** (with hundreds or less documents), set this parameter to `False` and only work with the words \"seen\" during training (aka, your \"vocabulary\").\n",
    "\n",
    "\n",
    "#### `sublinear_tf`, Boolean, `False`\n",
    "\n",
    "Apply logarthimic scaling to term frequencies.\n",
    "Ironically, the default is `False`, but you almost always want to use this (*except* for **tiny training datasets** with dozens or less documents, *possibly* (do evaluate that assumption first!)), so remember to **use `True` by default**!\n",
    "\n",
    "### Setup and vecorization\n",
    "\n",
    "With the above in mind, a good default setup for our corpus \"dimensions\" might be (feel free to play around with this setup and see if you can increase the end-result!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.17 s, sys: 135 ms, total: 5.31 s\n",
      "Wall time: 5.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    min_df=3,\n",
    "    max_df=0.5,\n",
    "    #ngram_range=(1,2),\n",
    "    #token_pattern='(?u)(?:\\\\b|(?<=_))[^\\W_][^\\W_]+(?:\\\\b|(?=_))',\n",
    ")\n",
    "# train (aka, fit) the vectorizer to the train data:\n",
    "X_train = vectorizer.fit_transform(train.data)\n",
    "# with the fitted vectorizer, transform the test data:\n",
    "X_test = vectorizer.transform(test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your machine isn't very old, all this \"text-to-numbers\" (vectors) setup should have not taken more than a few seconds.\n",
    "The most time-consuming factor in the above setup is the n-gram range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89i\n",
      "9607\n",
      "9999\n",
      "c5i7j7\n",
      "c5jobh\n",
      "completely\n",
      "detached\n",
      "domi\n",
      "donot\n",
      "existance\n",
      "flops\n",
      "freeze\n",
      "gateway2000\n",
      "gentleman\n",
      "pmoloney\n",
      "presentation\n",
      "proximity\n",
      "socketed\n",
      "thibedeau\n",
      "w4w\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# look at some random features (\"terms\")\n",
    "print(\"\\n\".join(sorted(random.sample(feature_names, 20))))\n",
    "#print()\n",
    "# first n\n",
    "#print(\"\\n\".join(feature_names[:5]))\n",
    "# last n\n",
    "#print(\"\\n\".join(feature_names[-5:]))\n",
    "# problems:\n",
    "#print()\n",
    "#print(\"\\n\".join(n for n in feature_names if \"__\" in n))\n",
    "\n",
    "feature_names = np.asarray(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this frequently enough, you should see that tokenization could have been a bit better... E.g., filtering out all the underscore \"separators\". Also, sorting your features and looking at the first/last few also often helps identify issues (here, e.g., all the numbers, which might lead to overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying text\n",
    "\n",
    "Now that we have a numeric representation of our text, let's train some machine learning classifiers with it!\n",
    "As the remainder of the ASDM courses should give you a pretty good feeling for most of the issues of classifiers, we won't detail their setups here.\n",
    "Also, SciKit-Learn itself has pretty good [tutorials](http://scikit-learn.org/stable/supervised_learning.html) on all these (supervised) classifiers.\n",
    "\n",
    "The following setup and test-run accross a whole bunch of possible classifiers is taken from the [SciKit-Learn document classification tutorial](http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html), with some minor modifications and updates (to remove deprecation warnings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn import metrics # yay, evaluation metrics! (day 5...)\n",
    "from sklearn.utils.extmath import density\n",
    "\n",
    "def trim(s):\n",
    "    \"\"\"\n",
    "    Trim string to fit on terminal\n",
    "    (assuming 80-column display)\n",
    "    \"\"\"\n",
    "    \n",
    "    return s if len(s) <= 80 else s[:77] + \"...\"\n",
    "\n",
    "def benchmark(clf, adjustments=\"\"):\n",
    "    print(adjustments, end=\"\")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"\\ntrain time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d features\" %\n",
    "              clf.coef_.shape[1])\n",
    "        print(\"density: %.1f%% of features used\" % \n",
    "              (100 * density(clf.coef_)))\n",
    "        #print(\"\\ntop 10 keywords per class:\")\n",
    "        #for i, label in enumerate(train.target_names):\n",
    "        #    top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        #    print(trim(\"%s: %s\" % (\n",
    "        #        label, \" \".join(feature_names[top10])\n",
    "        #    )))\n",
    "\n",
    "    #print(\"\\nclassification report:\")\n",
    "    #print(metrics.classification_report(y_test, pred,\n",
    "    #                                    target_names=train.target_names))\n",
    "    #print(\"\\nconfusion matrix:\")\n",
    "    #print(metrics.confusion_matrix(y_test, pred))\n",
    "    \n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return adjustments + clf_descr, score, train_time, test_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a wide selection of supervised classifiers (this might take a few minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=None, solver='sag',\n",
      "        tol=0.01)\n",
      "\n",
      "train time: 10.238s\n",
      "test time:  0.044s\n",
      "accuracy:   0.860\n",
      "dimensionality: 38842 features\n",
      "density: 100.0% of features used\n",
      "\n",
      "================================================================================\n",
      "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      n_iter=50, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
      "      verbose=0, warm_start=False)\n",
      "\n",
      "train time: 8.330s\n",
      "test time:  0.062s\n",
      "accuracy:   0.807\n",
      "dimensionality: 38842 features\n",
      "density: 25.2% of features used\n",
      "\n",
      "================================================================================\n",
      "PassiveAggressiveClassifier(C=1.0, class_weight=None, fit_intercept=True,\n",
      "              loss='hinge', n_iter=50, n_jobs=1, random_state=None,\n",
      "              shuffle=True, verbose=0, warm_start=False)\n",
      "\n",
      "train time: 12.305s\n",
      "test time:  0.062s\n",
      "accuracy:   0.849\n",
      "dimensionality: 38842 features\n",
      "density: 69.9% of features used\n",
      "\n",
      "================================================================================\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "train time: 0.022s\n",
      "test time:  14.288s\n",
      "accuracy:   0.721\n",
      "\n",
      "================================================================================\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\n",
      "train time: 48.866s\n",
      "test time:  1.574s\n",
      "accuracy:   0.766\n",
      "\n",
      "================================================================================\n",
      "L2 LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
      "     verbose=0)\n",
      "\n",
      "train time: 5.908s\n",
      "test time:  0.037s\n",
      "accuracy:   0.856\n",
      "dimensionality: 38842 features\n",
      "density: 100.0% of features used\n",
      "\n",
      "================================================================================\n",
      "L2 SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "train time: 7.323s\n",
      "test time:  0.038s\n",
      "accuracy:   0.855\n",
      "dimensionality: 38842 features\n",
      "density: 62.3% of features used\n",
      "\n",
      "================================================================================\n",
      "L1 LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
      "     verbose=0)\n",
      "\n",
      "train time: 11.155s\n",
      "test time:  0.034s\n",
      "accuracy:   0.826\n",
      "dimensionality: 38842 features\n",
      "density: 0.9% of features used\n",
      "\n",
      "================================================================================\n",
      "L1 SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "train time: 17.310s\n",
      "test time:  0.036s\n",
      "accuracy:   0.791\n",
      "dimensionality: 38842 features\n",
      "density: 0.4% of features used\n",
      "\n",
      "================================================================================\n",
      "Elastic SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "\n",
      "train time: 28.579s\n",
      "test time:  0.036s\n",
      "accuracy:   0.850\n",
      "dimensionality: 38842 features\n",
      "density: 9.7% of features used\n",
      "\n",
      "================================================================================\n",
      "NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "\n",
      "train time: 0.233s\n",
      "test time:  0.057s\n",
      "accuracy:   0.780\n",
      "\n",
      "================================================================================\n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "train time: 0.104s\n",
      "test time:  0.041s\n",
      "accuracy:   0.836\n",
      "dimensionality: 38842 features\n",
      "density: 100.0% of features used\n",
      "\n",
      "================================================================================\n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "train time: 0.117s\n",
      "test time:  0.079s\n",
      "accuracy:   0.761\n",
      "dimensionality: 38842 features\n",
      "density: 100.0% of features used\n",
      "\n",
      "================================================================================\n",
      "SVC Pipeline(steps=[('feature_selection', SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
      "     verbose=0),\n",
      "        prefit=False, thresho...ax_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0))])\n",
      "\n",
      "train time: 12.350s\n",
      "test time:  0.061s\n",
      "accuracy:   0.842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The SciKit-Learn data processing pipeline:\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Using classifiers as feature selectors (in a pipeline):\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"sag\"),\n",
    "         \"Ridge Classifier\"),\n",
    "        (Perceptron(n_iter=50),\n",
    "         \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(n_iter=50),\n",
    "         \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=10),\n",
    "         \"kNN\"),\n",
    "        (RandomForestClassifier(n_estimators=100),\n",
    "         \"Random forest\")):\n",
    "    print('=' * 80)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "for penalty in [\"l2 \", \"l1 \"]:\n",
    "    \n",
    "    print('=' * 80)\n",
    "    classifier = LinearSVC(\n",
    "        loss='squared_hinge',\n",
    "        penalty=penalty.strip(),\n",
    "        dual=False,\n",
    "        tol=1e-3)\n",
    "    results.append(benchmark(classifier, penalty.upper()))\n",
    "    \n",
    "    print('=' * 80)\n",
    "    classifier = SGDClassifier(\n",
    "        alpha=.0001,\n",
    "        n_iter=50,\n",
    "        penalty=penalty.strip())\n",
    "    results.append(benchmark(classifier, penalty.upper()))\n",
    "\n",
    "print('=' * 80)\n",
    "classifier = SGDClassifier(\n",
    "    alpha=.0001,\n",
    "    n_iter=50,\n",
    "    penalty=\"elasticnet\")\n",
    "results.append(benchmark(classifier, \"Elastic \"))\n",
    "\n",
    "print('=' * 80)\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n",
    "print('=' * 80)\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "\n",
    "print('=' * 80)\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "\n",
    "print('=' * 80)\n",
    "classifier = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(\n",
    "      penalty=\"l1\",\n",
    "      dual=False,\n",
    "      tol=1e-3))),\n",
    "  ('classification', LinearSVC())\n",
    "])\n",
    "results.append(benchmark(classifier, adjustments=\"SVC \"))\n",
    "\n",
    "indices = np.arange(len(results))\n",
    "results = [[x[i] for x in results] for i in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Plot the historgrams of the classifier performance statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuMAAAI1CAYAAAB8GvSWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxGRckiRJ6onJuCRJktST/w87ismc6zcTLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117b40ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf_names, score, training_time, test_time = results\n",
    "training_time_norm = \\\n",
    "    np.array(training_time) / np.max(training_time)\n",
    "test_time_norm = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score, .25, label=\"score\", color='navy')\n",
    "plt.barh(indices + .25, training_time_norm, .2, \n",
    "         label=\"training time\", color='c')\n",
    "plt.barh(indices + .5, test_time_norm, .2,\n",
    "         label=\"test time\", color='darkorange')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i+0.1, c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In text mining, the classifier often matters much less than *everything else* (tokenization, n-grams, feature selection strategies, etc.), at least if you choose a fairly decent, but still rather simple classifier (here: multinoulli Naive Bayes).\n",
    "\n",
    "Another set of red flags are the test times of the Random Forset (think: gradient boosting, etc.) and kNN classifiers, i.e., their prohibitively large *prediction times* - at least if you are planning to deploy your classifier in settings where the time it takes to present results (to the user) matters, and the enormous training time of the Random Forest classifier (hint: a deep learning network would be even worse...)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
