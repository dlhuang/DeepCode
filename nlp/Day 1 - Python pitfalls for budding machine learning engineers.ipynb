{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python pitfalls for budding machine learning engineers\n",
    "\n",
    "Practical course material for the ASDM Class 09 (Text Mining) by Florian Leitner.\n",
    "\n",
    "Â© 2017 Florian Leitner. All rights reserved.\n",
    "\n",
    "## The `defaultdict`\n",
    "\n",
    "Quick review of Python's `deafultdict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# store integers mapped to some key\n",
    "demo = defaultdict(int)\n",
    "\n",
    "# NB: we did not define a value for 'defined'\n",
    "demo['defined'] = demo['defined'] + 1\n",
    "# But: it magically initialized itself!\n",
    "demo['missing'], demo['defined']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All practicality aside, this can become a trap, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'missing' in demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'really missing' in demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because, by fetching the value for `'missing'` (via `demo['missing']`), we actually added it to the mapping!\n",
    "\n",
    "## Banker's rounding & Notebook display precision\n",
    "\n",
    "In Python, explicit rounding doesn't behave like type casting and Integer divisions don't behave the same: Explicit rounding uses \"Banker's rounding\".\n",
    "\n",
    "Casting a `float` to an `int` (or using Integer divisions) chops off the decimals: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 2, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(1.01), int(1.5), int(1.99), int(2.5), int(3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But `round(float)` applies **Banker's rounding** (round down on even numbers, up on uneven ones):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 2, 2, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1.01), round(1.5), round(1.99), round(2.5), round(3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, if you therefore use **`round(float)`**, you get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[round(i/2) for i in range(1,5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you use **Integer division**, you get a *different result*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i//2 for i in range(1,5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are surprised by this behaviour; It is the standard [IEEE 754](https://en.wikipedia.org/wiki/Floating_point#Rounding_modes) behaviour for floating points that should be followed by all programming languages; It's languages that *don't* exhibit this bevahiour that are \"wrong\".\n",
    "\n",
    "With that in mind, Jupyter provides formating of floating-point numbers; e.g., scientific notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%e'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%precision %e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.364805e+04"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import pi\n",
    "pi**10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round to the last n digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.3f'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%precision 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that the notebooks uses **Banker's rounding**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.004, 2.003)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0035, 2.0035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `hash() != id()`\n",
    "\n",
    "The Python `hash` function generates an *almost* unique, \"identifying\" integer for any Python object.\n",
    "\n",
    "Calculate the (64 bit) integer hash of a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8360721101177055972"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `id` method Python uses internally to identify objects is not the same as `hash` and uses a smaller numerical range (because having a few billion objects in your Python process is rather unusal, so for performance reasons, `id` only hashes to 32 bits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4413646752"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilities and underflows\n",
    "\n",
    "Imagine we have the following three vectors of 100 probability scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probabilities = [\n",
    "    [1e-3]*100, [5e-4]*100, [1e-5]*100\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when hashing features, 32 bits might be too small, so be aware that `hash` gives you a larger range (\"more buckets\").\n",
    "\n",
    "To provide a possible, real-life setting, from where these vectors came from: E.g., we are trying to label a document with one of three different labels, and will assign it the label that has the highest joint probability over all features, e.g., a multinomial Naive Bayes classifier. So the three vectors above are the probabilities for each token and label pair, and we'll just assume we found the exact same probability for each token given the desired label (as that does not matter for our purposes).\n",
    "\n",
    "But due to that simplification, you can see immediately that the document should be assigned the first label, as that will have the highest joint probability ($0.001^{100} = \\prod{p_i}$)\n",
    ". \n",
    "\n",
    "So lets try calculate those joint probabilities for each of the three labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%e'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show us \"everything, please!\"\n",
    "%precision %e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.000000e-300"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(1e-3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000e+00"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(5e-4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000e+00"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(1e-5, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops! We quickly reached the limit of our processing capabilites; Two of the three cases give us a wrong answer (`P=0`).\n",
    "This problem is known as a (precision) **undeflow**.\n",
    "\n",
    "Now, this is typically remedied with a simple trick:\n",
    "Work in log-transformed space if working with probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.907755e+02"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = sum([log(1e-3)]*100); A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.600902e+02"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = sum([log(5e-4)]*100); B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.151293e+03"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = sum([log(1e-5)]*100); C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, though, that we cannot move back into normal space, as we'd still get a zero result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000e+00"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we calculate the actual probability for each of the three labels after this log transformation? I.e., how to get correct scores for each of the three labels, that, when summed up, equal 1? After all, we cannot transform the logs back directly, unless we end up with that pesky underflow issue again, as we just have seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.000000e+00, -6.931472e+01, -4.605170e+02]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logP = [A, B, C]\n",
    "#\n",
    "# Here is the critical trick: subtract the max. log(P_i) value from all values:\n",
    "#\n",
    "normP = [i - max(logP) for i in logP]\n",
    "#\n",
    "# Result:\n",
    "#\n",
    "normP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means, the normalized `log(P_i)` for the label[s] with the max. probablity now is scored with **zero**, and all other results are adjusted accordingly to this norm.\n",
    "\n",
    "Now we can safely transform the normalized scores back into correct probabilities. First, we reverse the log transformation of the scores (i.e., take the exponent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.000000e+00, 7.888609e-31, 1.000000e-200]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expP = list(map(exp, normP)); expP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Remark: Even if a probabilty now would end up being zero, you can under normal circumstances happily ignore that last underflow, as that label's probablity is so infinitesimaly small that for all practical concerns it can indeed be considered zero.)\n",
    "\n",
    "Finally we can calculate the true label probabilities from the proportions we just found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.000000e+00, 7.888609e-31, 1.000000e-200]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = [i / sum(expP) for i in expP]; P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is still a tiny margin of floating-point error in this result, becasue the above result is strictly summed up, does not sum to one, but to slightly more than one.\n",
    "\n",
    "But that's not even testable (It might become noticable if the three label probabilities had been a closer call, although - so usually, you will need to check floating point numbers according to some precision you wish to ensure to make sure that your probabilities are all correct):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(P) == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, unless you care about being correct beyond this, the outcome you can handle this way is typically acceptable in most practical cases. But to conclude: Working with floating point is probably still harder than you think, even now... FWIW, banks therefore typically prefer to use integers only."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
